# AI Model Configuration
# Set to 'true' to use local/private OpenAI-compatible endpoints (Ollama, llama.cpp, etc.)
# Set to 'false' or leave empty to use OpenRouter cloud service
VITE_USE_LOCAL_MODEL=true

# OpenRouter API Key (required when VITE_USE_LOCAL_MODEL=false)
# Get your key from: https://openrouter.ai/keys
VITE_API_KEY=

# Local Endpoint Configuration (used when VITE_USE_LOCAL_MODEL=true)
# URL of your local OpenAI-compatible endpoint
VITE_LOCAL_ENDPOINT_URL=http://localhost:11434/v1

# Model name as recognized by your local endpoint
VITE_LOCAL_MODEL_NAME=llama3.1:8b

# Common local endpoint examples:
# Ollama: http://localhost:11434/v1
# llama.cpp server: http://localhost:8080/v1
# LM Studio: http://localhost:1234/v1
# Text Generation WebUI: http://localhost:5000/v1

# Backend Configuration (for local models)
# URL of your local LLM server (FastFlowLM, Ollama, etc.)
LOCAL_LLM_URL=http://localhost:11434

# Timeout for local LLM requests (in milliseconds)
LOCAL_LLM_TIMEOUT=300000

# Backend server port
PORT=3001

# CORS allowed origins (comma-separated)
ALLOWED_ORIGINS=http://localhost:8080,http://localhost:8081,http://localhost:3000
